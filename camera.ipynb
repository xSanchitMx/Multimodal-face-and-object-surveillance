{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8fea95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.25M/6.25M [00:05<00:00, 1.21MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No image FAISS index found, creating a new one...\n",
      "[INFO] No text FAISS index found, creating a new one...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from PIL import Image\n",
    "import json\n",
    "from datetime import datetime\n",
    "from explain import explain\n",
    "import clip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "import clip_encoder\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "alerts_file = \"alerts/alerts.json\"\n",
    "os.makedirs(\"alerts\", exist_ok=True)\n",
    "\n",
    "d = 512\n",
    "\n",
    "image_index_path = \"data/image_index.faiss\"\n",
    "text_index_path = \"data/text_index.faiss\"\n",
    "image_paths_path = \"data/image_paths.txt\"\n",
    "text_paths_path = \"data/text_paths.txt\"\n",
    "\n",
    "if os.path.exists(image_index_path) and os.path.exists(image_paths_path):\n",
    "    print(\"[INFO] Loading existing image FAISS index...\")\n",
    "    image_index = faiss.read_index(image_index_path)\n",
    "    with open(image_paths_path, \"r\") as f:\n",
    "        image_gallery_paths = f.read().splitlines()\n",
    "else:\n",
    "    print(\"[INFO] No image FAISS index found, creating a new one...\")\n",
    "    image_index = faiss.IndexFlatIP(d)\n",
    "    image_gallery_paths = []\n",
    "\n",
    "if os.path.exists(text_index_path) and os.path.exists(text_paths_path):\n",
    "    print(\"[INFO] Loading existing text FAISS index...\")\n",
    "    text_index = faiss.read_index(text_index_path)\n",
    "    with open(text_paths_path, \"r\") as f:\n",
    "        text_gallery_paths = f.read().splitlines()\n",
    "else:\n",
    "    print(\"[INFO] No text FAISS index found, creating a new one...\")\n",
    "    text_index = faiss.IndexFlatIP(d)\n",
    "    text_gallery_paths = []\n",
    "\n",
    "output_dir = \"data/cropped_objects\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "new_queries_file = \"data/new_queries.txt\"\n",
    "\n",
    "IMAGE_SIM_THRESHOLD = 0.7\n",
    "TEXT_SIM_THRESHOLD = 0.6\n",
    "\n",
    "def embed_text(text: str):\n",
    "    text_tokens = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    return text_features.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1 bottle, 133.7ms\n",
      "Speed: 16.8ms preprocess, 133.7ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 122.7ms\n",
      "Speed: 8.7ms preprocess, 122.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 1 bowl, 87.5ms\n",
      "Speed: 1.3ms preprocess, 87.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bottles, 88.1ms\n",
      "Speed: 1.5ms preprocess, 88.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 75.0ms\n",
      "Speed: 1.9ms preprocess, 75.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 78.5ms\n",
      "Speed: 1.3ms preprocess, 78.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 1 bowl, 72.7ms\n",
      "Speed: 2.2ms preprocess, 72.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 79.1ms\n",
      "Speed: 1.4ms preprocess, 79.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 1 bowl, 74.2ms\n",
      "Speed: 1.5ms preprocess, 74.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 78.0ms\n",
      "Speed: 1.4ms preprocess, 78.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 91.0ms\n",
      "Speed: 1.8ms preprocess, 91.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 81.9ms\n",
      "Speed: 1.8ms preprocess, 81.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 85.4ms\n",
      "Speed: 1.6ms preprocess, 85.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 81.9ms\n",
      "Speed: 2.4ms preprocess, 81.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 79.3ms\n",
      "Speed: 3.0ms preprocess, 79.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 82.9ms\n",
      "Speed: 3.9ms preprocess, 82.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 1 bowl, 78.7ms\n",
      "Speed: 1.5ms preprocess, 78.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 123.7ms\n",
      "Speed: 2.0ms preprocess, 123.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 84.1ms\n",
      "Speed: 2.7ms preprocess, 84.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 bottles, 100.5ms\n",
      "Speed: 1.6ms preprocess, 100.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 90.0ms\n",
      "Speed: 1.5ms preprocess, 90.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 bottles, 85.8ms\n",
      "Speed: 1.1ms preprocess, 85.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 73.7ms\n",
      "Speed: 1.4ms preprocess, 73.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 70.8ms\n",
      "Speed: 1.7ms preprocess, 70.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 71.2ms\n",
      "Speed: 1.6ms preprocess, 71.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_count = 0\n",
    "\n",
    "last_detected_image = None\n",
    "last_detected_text = None\n",
    "\n",
    "IMAGE_SIM_THRESHOLD = 1\n",
    "TEXT_SIM_THRESHOLD = 0.2\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model(frame)[0]\n",
    "\n",
    "    for i, box in enumerate(results.boxes.xyxy):\n",
    "        x1, y1, x2, y2 = map(int, box[:4])\n",
    "        cropped = frame[y1:y2, x1:x2]\n",
    "\n",
    "        timestamp = int(time.time() * 1000)\n",
    "        filename = f\"{output_dir}/crop_{frame_count}_{i}_{timestamp}.jpg\"\n",
    "        cv2.imwrite(filename, cropped)\n",
    "\n",
    "        emb = clip_encoder.embed_image(filename).astype(\"float32\")\n",
    "        faiss.normalize_L2(emb)\n",
    "\n",
    "        image_match = None\n",
    "        text_match = None\n",
    "        \n",
    "        if image_index.ntotal > 0:\n",
    "            D_img, I_img = image_index.search(emb, k=1)\n",
    "            img_score = D_img[0][0]\n",
    "            img_idx = I_img[0][0]\n",
    "            \n",
    "            if img_score >= IMAGE_SIM_THRESHOLD:\n",
    "                image_match = {\n",
    "                    'path': image_gallery_paths[img_idx],\n",
    "                    'score': img_score,\n",
    "                    'type': 'image'\n",
    "                }\n",
    "\n",
    "        if text_index.ntotal > 0:\n",
    "            D_txt, I_txt = text_index.search(emb, k=1)\n",
    "            txt_score = D_txt[0][0]\n",
    "            txt_idx = I_txt[0][0]\n",
    "            \n",
    "            if txt_score >= TEXT_SIM_THRESHOLD:\n",
    "                text_match = {\n",
    "                    'path': text_gallery_paths[txt_idx],\n",
    "                    'score': txt_score,\n",
    "                    'type': 'text'\n",
    "                }\n",
    "\n",
    "        y_offset = y1 - 10\n",
    "        \n",
    "        if image_match:\n",
    "            person_id = os.path.basename(image_match['path'])\n",
    "            text = f\"IMG: {person_id} ({image_match['score']:.2f})\"\n",
    "            color = (0, 0, 255)\n",
    "            \n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, text, (x1, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            y_offset -= 25\n",
    "            \n",
    "            if person_id != last_detected_image:\n",
    "                last_detected_image = person_id\n",
    "                \n",
    "                filename_only = f\"img_{person_id}_{int(time.time())}.jpg\"\n",
    "                explained_img = os.path.join(\"alerts\", filename_only)\n",
    "                \n",
    "                explain(filename, image_match['path'], explained_img)\n",
    "                \n",
    "                alert = {\n",
    "                    \"type\": \"image\",\n",
    "                    \"person\": person_id,\n",
    "                    \"time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"image\": filename_only,\n",
    "                    \"score\": float(image_match['score'])\n",
    "                }\n",
    "                \n",
    "                if os.path.exists(alerts_file):\n",
    "                    with open(alerts_file, \"r\") as f:\n",
    "                        alerts = json.load(f)\n",
    "                else:\n",
    "                    alerts = []\n",
    "                \n",
    "                alerts.append(alert)\n",
    "                with open(alerts_file, \"w\") as f:\n",
    "                    json.dump(alerts, f, indent=4)\n",
    "        \n",
    "        if text_match:\n",
    "            text_desc = text_match['path']\n",
    "            text = f\"TXT: {text_desc} ({text_match['score']:.2f})\"\n",
    "            color = (255, 0, 0)\n",
    "            \n",
    "            if not image_match:\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            cv2.putText(frame, text, (x1, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            y_offset -= 25\n",
    "            \n",
    "            if text_desc != last_detected_text:\n",
    "                last_detected_text = text_desc\n",
    "                \n",
    "                filename_only = f\"txt_{int(time.time())}.jpg\"\n",
    "                saved_img = os.path.join(\"alerts\", filename_only)\n",
    "                cv2.imwrite(saved_img, cropped)\n",
    "                \n",
    "                alert = {\n",
    "                    \"type\": \"text\",\n",
    "                    \"person\": text_desc,\n",
    "                    \"time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"image\": filename_only,\n",
    "                    \"score\": float(text_match['score'])\n",
    "                }\n",
    "                \n",
    "                if os.path.exists(alerts_file):\n",
    "                    with open(alerts_file, \"r\") as f:\n",
    "                        alerts = json.load(f)\n",
    "                else:\n",
    "                    alerts = []\n",
    "                \n",
    "                alerts.append(alert)\n",
    "                with open(alerts_file, \"w\") as f:\n",
    "                    json.dump(alerts, f, indent=4)\n",
    "        \n",
    "        if not image_match and not text_match:\n",
    "            text = \"No match\"\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    if os.path.exists(new_queries_file):\n",
    "        with open(new_queries_file, \"r\") as f:\n",
    "            new_queries = f.read().splitlines()\n",
    "\n",
    "        if new_queries:\n",
    "            for q in new_queries:\n",
    "                if q.startswith(\"img:\"):\n",
    "                    q_path = q.replace(\"img:\", \"\").strip()\n",
    "                    if os.path.exists(q_path):\n",
    "                        print(f\"Adding new IMAGE query: {q_path}\")\n",
    "                        emb = clip_encoder.embed_image(q_path).astype(\"float32\")\n",
    "                        faiss.normalize_L2(emb)\n",
    "                        image_index.add(emb)\n",
    "                        image_gallery_paths.append(q_path)\n",
    "\n",
    "                elif q.startswith(\"txt:\"):\n",
    "                    q_text = q.replace(\"txt:\", \"\").strip()\n",
    "                    if q_text:\n",
    "                        print(f\"Adding new TEXT query: '{q_text}'\")\n",
    "                        emb = embed_text(q_text).astype(\"float32\")\n",
    "                        faiss.normalize_L2(emb)\n",
    "                        text_index.add(emb)\n",
    "                        text_gallery_paths.append(q_text)\n",
    "\n",
    "            faiss.write_index(image_index, image_index_path)\n",
    "            faiss.write_index(text_index, text_index_path)\n",
    "            \n",
    "            with open(image_paths_path, \"w\") as f:\n",
    "                f.write(\"\\n\".join(image_gallery_paths))\n",
    "            \n",
    "            with open(text_paths_path, \"w\") as f:\n",
    "                f.write(\"\\n\".join(text_gallery_paths))\n",
    "\n",
    "            open(new_queries_file, \"w\").close()\n",
    "\n",
    "    cv2.imshow(\"Live-Feed\", frame)\n",
    "    frame_count += 1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41952f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
